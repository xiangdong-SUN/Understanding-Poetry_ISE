{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'util'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-730bc08921e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_num_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_word2idx_idx2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_embedding_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextDatasetWithGloveElmoSuffix\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mTextDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRNNSequenceClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'util'"
     ]
    }
   ],
   "source": [
    "from util import get_num_lines, get_vocab, embed_sequence, get_word2idx_idx2word, get_embedding_matrix\n",
    "from util import TextDatasetWithGloveElmoSuffix as TextDataset\n",
    "from util import evaluate\n",
    "from model import RNNSequenceClassifier\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.Optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import csv\n",
    "import h5py\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "print(\"Pytorch version:\")\n",
    "print(torch.__version__)\n",
    "print(\"GPU Detected:\")\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data pre-processing\n",
    "\n",
    "get raw dataset as a list:\n",
    "    each element is a triple:\n",
    "        a sentence : string\n",
    "        a index: int: idx of the focus verb\n",
    "        a label: int 1 or 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_poetry = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-0227eb352c73>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-0227eb352c73>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    with\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "with open('../dataset/Poetry...') as f:\n",
    "    lines = csv.reader(f)\n",
    "    next(lines)\n",
    "    for line in lines:\n",
    "        raw_poetry.append([line[1].strip(), int(line[2]), int(line[3])])\n",
    "    print('Poetry dataset size:', len(raw_poetry))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 get vocabulary and glove embeddings in raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../glove/glove840B300d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0c6ee041eee0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_word2idx_idx2word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# glove_embeddings a nn.Embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mglove_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embedding_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# elmo_emnbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# set elmos_poetry = None to exclude elmo vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/metaphor-in-context-master/classification/util.py\u001b[0m in \u001b[0;36mget_embedding_matrix\u001b[0;34m(word2idx, idx2word, normalization)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mglove_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../glove/glove840B300d.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mglove_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mglove_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_num_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0msplit_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../glove/glove840B300d.txt'"
     ]
    }
   ],
   "source": [
    "# vocab is a set of words\n",
    "vocab = get_vocab(raw_poetry)\n",
    "# two dictionaries. <PAD>:0, <UNK>:1\n",
    "word2idx, idx2word = get_word2idx_idx2word(vocab)\n",
    "# glove_embeddings a nn.Embeddings\n",
    "glove_embeddings = get_embedding_matrix(word2idx, idx2word, normalization=False)\n",
    "# elmo_emnbeddings\n",
    "# set elmos_poetry = None to exclude elmo vectors\n",
    "elmos_poetry = h5py.File('../elmo/Trofi3737.hdf5', 'r')\n",
    "#suffix_embeddings: number of suffix tag is 2, and the suffix embedding dimension is 50\n",
    "suffix_embeddings = nn.Embedding(2, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 embed the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4c7a50818251>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_poetry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m embedded_poetry = [[embed_sequence(example[0], example[0], example[1],\n\u001b[1;32m      5\u001b[0m                                    word2idx, elmo_poetry, suffix_embeddings), \n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "random.schuffle(raw_poetry)\n",
    "\n",
    "embedded_poetry = [[embed_sequence(example[0], example[0], example[1],\n",
    "                                   word2idx, elmo_poetry, suffix_embeddings), \n",
    "                    example[2]] for example in raw_poetry]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 set up Dataloader for batching\n",
    "\n",
    "10-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedded_poetry' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d9848eaf6b9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# separate the embedded_sentences and labels into 2 list,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# in order to pass into the TextDataset as argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0membedded_poetry\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0membedded_poetry\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# ten_folds is a list of 10 tuples, each tuple is (list_of_embedded_sentences, list_of_corresponding_labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embedded_poetry' is not defined"
     ]
    }
   ],
   "source": [
    "# separate the embedded_sentences and labels into 2 list, \n",
    "# in order to pass into the TextDataset as argument\n",
    "sentences = [example[0] for example in embedded_poetry]\n",
    "labels = [example[1] for example in embedded_poetry]\n",
    "# ten_folds is a list of 10 tuples, each tuple is (list_of_embedded_sentences, list_of_corresponding_labels)\n",
    "ten_folds = []\n",
    "\n",
    "\n",
    "fold_size = int(3737/10)\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    ten_folds.append((sentences[i*fold_size:(i+1)*fold_size], \n",
    "                      labels[i*fold_size:(i+1)*fold_size]))\n",
    "\n",
    "optimal_fls = []\n",
    "optimal_ps = []\n",
    "optimal_rs = []\n",
    "optimal_accs = []\n",
    "predictions_all = []\n",
    "for i in range(10):\n",
    "    '''\n",
    "    2.3 set up dataloader for batching\n",
    "    '''\n",
    "    training_sentences = []\n",
    "    training_labels = []\n",
    "    for j in range(10):\n",
    "        if j != i:\n",
    "            training_sentences.extend(ten_folds[j][0])\n",
    "            training_labels.extend(ten_folds[j][1])\n",
    "    training_dataset_poetry = TextDataset(training_sentences, training_labels)\n",
    "    val_dataset_poetry = TextDataset(ten_folds[i][0], ten_folds[i][1])\n",
    "    \n",
    "    #Data-related hyperparameters\n",
    "    batch_size = 10\n",
    "    # set up a DataLoader for the training, validation, and test dataset\n",
    "    train_dataloader_poetry = DataLoader(dataset=training_dataset_poetry, batch_size=batch_size, shuffle=True, collate_fn=TextDataset.collate_fn)\n",
    "    val_dataloader_poetry = DataLoader(dataset=val_dataset_poetry,batch_size=batch_size,shuffle=False, collate_fn=TextDataset.collate_fn)\n",
    "    '''3. model training\n",
    "    3.1 set up model, loss criterion, optimizer\n",
    "    '''\n",
    "    # Instantiate the model\n",
    "    # embedding_dim = glove + elmo + shuffix indicator\n",
    "    # dropout1: dropout on input to RNN\n",
    "    # dropout2: fropout in RNN; would be used if num_layer = 1\n",
    "    # dropout3: fropout on hidden state of RNN to linear layer\n",
    "    rnn_clf = RNNSequenceClassifier(num_classes=2, embedding_dim=300+1024+50, hidden_size=300,\n",
    "                                   num_layers=1, bidir=True, dropout1=0.2, dropout2=0, dropout3=0)\n",
    "    # move the model to the GPU if available\n",
    "    if using_GPU:\n",
    "        rnn_clf = rnn_clf.cuda()\n",
    "    # set up criterion for calculating loss\n",
    "    nll_criterion = nn.NLLLoss()\n",
    "    # set up optimizer for uodating the parameters of the rnn_clf\n",
    "    rnn_clf_optimizer = optim.Adam(rnn_clf.paramters(), lr=0.001)\n",
    "    # number of epochs (passes through the dataset) to train the model for\n",
    "    num_epochs = 15\n",
    "    \n",
    "    '''\n",
    "    3.2 train model\n",
    "    '''\n",
    "    training_loss = []\n",
    "    val_loss = []\n",
    "    training_f1 = []\n",
    "    val_f1 = []\n",
    "    val_p = []\n",
    "    val_r = []\n",
    "    val_acc = []\n",
    "    # a counter for the number of gradient updates\n",
    "    num_iter = 0\n",
    "    train_dataloader = train_dataloader_poetry\n",
    "    val_dataloder = val_dataloader_poetry\n",
    "    model_index = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Starting epoch{}\".format(epoch + 1))\n",
    "        for (example_text, example_lengths, labels) in train_dataloader:\n",
    "            example_text = Variable(example_text)\n",
    "            example_lengths = Varibale(example_lengths)\n",
    "            labels = Variable(labels)\n",
    "            if using_GPU:\n",
    "                example_text = example_text.cuda()\n",
    "                example_lengths = example_lengths.cuda()\n",
    "                labels = labels.cuda()\n",
    "            # predicted shape: (batch_size, 2)\n",
    "            predicted = rnn_clf(example_text, example_lengths)\n",
    "            batch_loss = nll_criterion(predicted, labels)\n",
    "            rnn_clf_optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            rnn_clf_optimizer.step()\n",
    "            num_iter += 1\n",
    "            # calculate validation and training set loss and accuracy every 200 gradient updates\n",
    "            if num_iter % 200 ==0:\n",
    "                avg_eval_loss, eval_accuracy, precision, recall, f1, fus_f1 = evaluate(val_dataloader, rnn_clf, nll_criterion, using_GPU)\n",
    "                val_loss.append(avg_eval_loss)\n",
    "                val_f1.append(f1)\n",
    "                val_p.append(precision)\n",
    "                val_r.append(recall)\n",
    "                val_acc.append(eval_accuracy)\n",
    "                print(\n",
    "                \"Iteration {}. Validation Loss {}. Validation Accuracy {}. Validation Precesion {}. Validation Recall {}. Validation F1 {}. Validation class-wise F1 {}.\".format(\n",
    "                num_iter, avg_eval_loss, eval_accuracy, precession, recall, f1, fus_f1)\n",
    "                )\n",
    "# filename = '../models/LSTMSuffixElmoAtt_MOH_fold_' + str(i) + '_epoch_' + str(model_index) + '.pt'\n",
    "# torch.save(rnn_clf, filename)\n",
    "                model_index += 1\n",
    "# avg_eval_loss, eval_accuracy, precision, recall, f1, fus_f1 = evaluate(train_dataloader, rnn_clf, nll_criterion, using_GPU)\n",
    "# training_loss.append(avg_eval_loss)\n",
    "# training_f1.append(f1)\n",
    "# print(\n",
    "# \"Iteration {}. Training Loss {}. Training Accuracy {}. Training Precision {}. Training Recall {}. Training F1 {}. Training class-wise F1 {}.\".format(\n",
    "#  num_iter, avg_eval_loss, eval_accuracy, precision, recall, f1, fus_f1))      \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    additional trianing!\n",
    "    \"\"\"\n",
    "#     rnn_clf_optimizer = optim.Adam(rnn_clf.parameters(), lr=0.0005)\n",
    "#     for epoch in range(num_epochs):\n",
    "#         print(\"Starting epoch {}\".format(epoch + 1))\n",
    "#         for (example_text, example_lengths, labels) in train_dataloader:\n",
    "#             example_text = Variable(example_text)\n",
    "#             example_lengths = Variable(example_lengths)\n",
    "#             labels = Variable(labels)\n",
    "#             if using_GPU:\n",
    "#                 example_text = example_text.cuda()\n",
    "#                 example_lengths = example_lengths.cuda()\n",
    "#                 labels = labels.cuda()\n",
    "#             # predicted shape: (batch_size, 2)\n",
    "#             predicted = rnn_clf(example_text, example_lengths)\n",
    "#             batch_loss = nll_criterion(predicted, labels)\n",
    "#             rnn_clf_optimizer.zero_grad()\n",
    "#             batch_loss.backward()\n",
    "#             rnn_clf_optimizer.step()\n",
    "#             num_iter += 1\n",
    "#             # Calculate validation and training set loss and accuracy every 200 gradient updates\n",
    "#             if num_iter % 100 == 0:\n",
    "#                 avg_eval_loss, eval_accuracy, precision, recall, f1, fus_f1 = evaluate(val_dataloader, rnn_clf, nll_criterion, using_GPU)\n",
    "#                 val_loss.append(avg_eval_loss)\n",
    "#                 val_f1.append(f1)\n",
    "#                 val_p.append(precision)\n",
    "#                 val_r.append(recall)\n",
    "#                 val_acc.append(eval_accuracy)\n",
    "#                 print(\n",
    "#                     \"Iteration {}. Validation Loss {}. Validation Accuracy {}. Validation Precision {}. Validation Recall {}. Validation F1 {}. Validation class-wise F1 {}.\".format(\n",
    "#                         num_iter, avg_eval_loss, eval_accuracy, precision, recall, f1, fus_f1))\n",
    "#                 model_index += 1\n",
    "    \n",
    "    print(\"Training done for fold {}\".format(i))\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    3.3\n",
    "    plot the training process: MET F1 and losses for validation and training dataset\n",
    "    \"\"\"\n",
    "#     plt.figure(0)\n",
    "#     plt.title('F1 for TroFI dataset on fold ' + str(i))\n",
    "#     plt.xlabel('iteration (unit:200)')\n",
    "#     plt.ylabel('F1')\n",
    "#     plt.plot(val_f1,'g')\n",
    "#     plt.plot(val_p,'r')\n",
    "#     plt.plot(val_r,'b')\n",
    "#     plt.plot(val_acc,'c')\n",
    "#     plt.plot(training_f1, 'b')\n",
    "#     plt.legend(['Validation F1', 'Validation precision', 'validaiton recall', 'validation accuracy', 'Training F1'], loc='upper right')\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "#     plt.figure(1)\n",
    "#     plt.title('Loss for TroFi dataset on fold ' + str(i))\n",
    "#     plt.xlabel('iteration (unit:200)')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.plot(val_loss,'g')\n",
    "#     plt.plot(training_loss, 'b')\n",
    "#     plt.legend(['Validation loss', 'Training loss'], loc='upper right')\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    store the best f1\n",
    "    \"\"\"\n",
    "    print('val_f1: ', val_f1)\n",
    "    idx = 0\n",
    "    if math.isnan(max(val_f1)):\n",
    "        optimal_f1s.append(max(val_f1[6:]))\n",
    "        idx = val_f1.index(optimal_f1s[-1])\n",
    "        optimal_ps.append(val_p[idx])\n",
    "        optimal_rs.append(val_r[idx])\n",
    "        optimal_accs.append(val_acc[idx])\n",
    "    else:\n",
    "        optimal_f1s.append(max(val_f1))\n",
    "        idx = val_f1.index(optimal_f1s[-1])\n",
    "        optimal_ps.append(val_p[idx])\n",
    "        optimal_rs.append(val_r[idx])\n",
    "        optimal_accs.append(val_acc[idx])\n",
    "#     filename = '../models/LSTMSuffixElmoAtt_TroFi_fold_' + str(i) + '_epoch_' + str(idx) + '.pt'\n",
    "#     temp_model = torch.load(filename)\n",
    "#     print('best model: ', filename)\n",
    "#     predictions_all.extend(test(val_dataloader_TroFi, temp_model, using_GPU))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('F1 on Poetry by 10-fold = ', optimal_fls)\n",
    "print('Precision on Poetry = ', np.mean(np.array(optimal_ps)))\n",
    "print('Recall on Poetry = ', np.mean(np.array(optimal_rs)))\n",
    "print('F1 on Poetry = ', np.mean(np.array(optimal_fls)))\n",
    "print('Accuracy on POetry =', np.mean(np.array(optimal_accs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(2)\n",
    "# plt.title('F1 for TroFi dataset on ten folds')\n",
    "# plt.xlabel('fold')\n",
    "# plt.ylabel('F1')\n",
    "# plt.plot(optimal_ps,'r')\n",
    "# plt.plot(optimal_rs,'b')\n",
    "# plt.plot(optimal_f1s,'g')\n",
    "# plt.plot(optimal_accs,'c')\n",
    "# plt.plot([np.mean(np.array(optimal_f1s))] * 10, 'y')\n",
    "# plt.legend(['precision for each fold', 'recall for each fold', 'F1 for each fold', 'accuracy for each fold', 'Average F1'], loc='upper right')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
